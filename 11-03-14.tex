\subsection{Zusammenfassung}
\subsubsection{Das Differenzial}
$f:\Omega\to\mb{R}$, $\Omega\subset\mb{R}^n$, Umgebung von $x$.
\[f\s\text{diff in $x$}\iff \exists L:\mb{R}^n\to\mb{R}\s\text{linear s.d.}\]
\begin{equation}
  \label{e:1103141}
  \lim_{h\downarrow 0}\frac{f(x+h)-f(x)-L(h)}{\Norm{h}}=0
\end{equation}
(Zur Erinnunerung:
\[\lim_{h\downarrow 0}G(h)=0\iff \left(\forall \varepsilon>0\; \exists \delta>0\;: 0<\Norm{h}<\delta\implies\Abs{G(h)}<\varepsilon\right)\, \]
\[\iff \left(\forall \text{ Folgen } \{h_k\} \mbox{ die $\neq 0$ aber $\to 0$, es gilt } G(h_k)\to 0\right)\quad \Big)\]
Wenn $f$ differenzierbar ist und \eqref{e:1103141} erfüllt, heisst $L$ das Differential von $f$ an
der Stelle $x$:
\[L=\md f|_x\, .\]
\subsubsection{Richtungsableitung}
$x\in\Omega$, $h\in\mb{R}^m$, $g(t)=f(x+th)$ (wohldefiniert für $\Abs{t}$ klein)
\[\partial_h f(x)=g'(0)=\Limo{t}\frac{f(x+th)-f(x)}{t}\]
\subsubsection{Partielle Ableitung}
$(x_1,\cdots,x_n)$ Koordinaten in $\mb{R}^n$. $y\in \Omega$ so dass $\Omega$ eine Umgebung von $y$ ist.
Dann
\[\Part{f}{x_i}(y)\left( =\partial_{x_i}f(y) \right)=\Limo{t}\frac{f (y_1,\dots,y_i+t,\dots,y_n-f(y))}{t}\]
Falls $e_i=(0,\dots,0,\underbrace{1}_{i\text{ Stelle}},0,\dots,0)$
\[\Part{f}{x_i}(y)=\Limo{t}\frac{f(y+te_i)-f(y)}{t}=\partial_{e_i}f(y)\]

\subsection{Das Hauptkriterium der Differenzierbarkeit}

Die Existenz der Richtungsableitungen genugt nicht f\"ur die Differenzierbarkeit von $f$.
Deswegen die Existenz der partiellen Ableitungen (d.h. von {\em manchen} Richtungsableitungen)
impliziert {\bf nicht} die Differenzierbarkeit.

\begin{Sat}
  (Hauptkriterium der Differenzierbarkeit) Sei $f:U\to \mb{R}$ und $U$ eine Umgebung von $y$. Falls $\Part{f}{x_1},\dots,\Part{f}{x_n}$ \ul{in $U$} existieren und stetig \ul{in $y$} sind, dann ist $f$ in $y$ differenzierbar.
\end{Sat}
\begin{Bem}
Aber {\em Vorsicht}: die Differenzierbarkeit von $f$ impliziert {\bf nicht} die Stetigkeit der 
partiellen Ableitungen!
\end{Bem}

\begin{Bew}
  $h=(h_1,\dots,h_n)\in\mb{R}^n.$ Wir setzten
  \[L(h)=\sum^n_{i=1}\Part{f}{x_i}(y)h_i\, .\]
  \paragraph{Ziel} $L$ ist das Differential von $f$, d.h.
\begin{equation}\label{e:Ziel}
\Limo{h}\frac{f(x+h)-f(x)-L(h)}{\Norm{h}}=0
\end{equation}
Wir schreiben
\begin{eqnarray}
f(x+h)-f(x)&=& f(x+(h_1,\dots,h_n))-f(y+(h_1,\dots,h_{n-1},0)\nonumber\\
&&+f(y+(h_1,\dots,h_{n-1}, 0)-f (y + (h_1, \ldots, h_{n-2}, 0, 0)\nonumber\\
&&+\ldots\nonumber\\
&&+ f(y + (h_1, \ldots, h_i, 0, \ldots 0) - f (y + (h_1, \ldots, h_{i-1}, 0, 0, \ldots 0) \qquad(\text{$i$te Zeile})\nonumber\\
&&+\ldots\nonumber\\
&&+f(y+(h_1,0,\dots,0))-f(y)\label{e:viele_Zeilen}
 \end{eqnarray}
Sei nun $g_i (t))=f(y+(h_1,\dots,h_{i-1},th_i,0,\dots,0)$. Die
$i$te Zeile in \eqref{e:viele_Zeilen} ist dann $g_i(1)-g_i(0)$. Aber 
  \[g_i'(t)=\Limo{\varepsilon}\frac{g_i(t+\varepsilon)-g_i(t)}{\varepsilon}\]
  \[=h_i\Limo{\varepsilon}\frac{f(y_1+h_1,\dots,y_{i-1},y_i+(t+\varepsilon)h_i,y_{i+1}\dots,y_n)-f(y_1+h_1,\dots,y_i+th_i,\dots,y_n}{\varepsilon h_i}\]
  \[=h_i\Part{f}{x_i}\left( y_1+h_1,\dots,y_i+th_i,y_{i+1},\dots,y_n \right)\, .\]
Deswegen die Existenz
der Richtungsableitungen in einer Ungebung von $y$ garantieren die Differenzierbarkeit
der Funktion $g_i$ falls $\|h\|$ klein genung ist. Aussserdem
\[
 \exists \xi_i\in \left[ 0,1 \right] \;: \qquad \text{$i$te Zeile von \eqref{e:viele_Zeilen}} =g_i'(\xi_i)\, 
\]
und so
 \begin{equation}
    \label{e:1103144}
\text{$i$te Zeile}=h_i\Part{f}{x_i}(y_1+h_1,\dots,y_{i-1}h_{i-1},y_i+\xi_ih_i,y_{i+1},\dots,y_n)
    =h_i\Part{f}{x_i}(y+\zeta_i)\, .
\end{equation}
wobei $\zeta_i=\left( h_1,\dots,h_{i-1},\xi h_i,0,\dots,0 \right)$
Wir setzten \eqref{e:1103144} in \eqref{e:viele_Zeilen}:
\[
f(y+h)-f(y)=\sum_{i=1}^nh_i\Part{f}{x_i}(y+\zeta_i)
\]
und deswegen
\begin{equation}
    \label{e:1103146}
f(x+h)-f(x)-L(h) = \sum_{i=1}^nh_i\left( \Part{f}{x_i}(y+\zeta_i)-\Part{f}{x_i}(y) \right)\, .
\end{equation}
Also,
\begin{equation}\label{e:1103147}
\frac{\Abs{f(x+h)-f(x)-L(h)}}{\Norm{h}}
  \;\stackrel{\eqref{e:1103146}}{\leq}\;\sum_{i=1}^n\frac{\Abs{h_i}\Abs{\Part{f}{x_i}(y+\zeta_i)-\Part{f}{x_i}(y)}}{\Norm{h}}
  \end{equation}
  Wenn $\Norm{h}\to 0$, $\Norm{\zeta_i}\to 0$. Die Stetigkeit von $\Part{f}{x_i}$ in $y$ impliziert
  \[\Part{f}{x_i}(y+\zeta_i)\to\Part{f}{x_i}\]
  Die rechte Seite von \eqref{e:1103147} $\to 0$ wenn $h\to 0$ $\implies$ \eqref{e:Ziel}.
\end{Bew}

\subsection{Die geometrische Bedeutung des Gradients}
Wir haben
  \[df|_{x_0}(h) = \partial_h f(x_0) =\sum_{i=1}^nh_i\Part{f}{x_i}(x_0)\
= \nabla f(x_0)\cdot h
\]
(manchmal wir schreiben auch $\langle \nabla f (x_0), h\rangle$).
Deswegen,
  \[\Abs{\partial_nf(x_0)}\stackrel{\text{Cauchy-Schwartz}}{\leq}\Norm{\nabla f(x_0)}\Norm{h}\]
  Falls $\Norm{h}=1$, dann
  \[\Abs{\partial_h f(x_0)}\leq\Norm{\nabla f(x_0)}\]
  Fall $\Norm{\nabla f(x_0)}\neq 0$, wir definieren
  \[K=\frac{\nabla f(x_0)}{\Norm{\nabla f(x_0)}}\, .\]
Dann $\Norm{K}=1$ und
  \[\partial_Kf(x_0)=\Norm{\nabla f(x_0)}\]
  Deswegen:
  \[K=\frac{\nabla f(x_0)}{\Norm{\nabla f(x_0)}}\]
  ist die Richtung der maximalen Steigung und
  \[\Norm{\nabla f(x_0)}\]
  ist die maximale Steigung.

\subsection{Rechenregeln}
\begin{Sat}\label{s:rechen_diff}]
  Sei $U$ eine Umgebung von $x\in\mb{R}^n$ und $f,g:U\to\mb{R}$ in $x$ differenzierbar. Dann sind $f+g$ und $fg$ auch differenzierbar in $x$ und
  \begin{equation}\label{e:summe}
\md (f+g)|_x=\md f|_x+\md g|x
\end{equation}
\begin{equation}\label{e:produkt}
\md(fg)=f(x)\md g|x+g(x)\md f|_x\, .
\end{equation}
  Falls $f(x)\neq 0$ ist auch $\frac{1}{f}$ in $x$ differenzierbar
\begin{equation}\label{e:bruch}
\md \left( \frac{1}{f} \right)|_x=-\frac{1}{(f(x))^2}\md f|_x\, .
\end{equation}
\end{Sat}
\begin{Kor}
  $g(x)\neq 0$, dann
  \[\md \left( \frac{f}{g} \right)|_x=\frac{1}{g(x)}\md f|_x-\frac{f(x)}{g(x)^2}\md g|_x\]
  \[=\frac{g(x)\md f|_x-f(x)\md g|_x}{g(x)^2}\]
\end{Kor}
\begin{Bew}[Beweis vom Satz \ref{s:rechen_diff}] Die Regel \eqref{e:summe} ist sehr einfach zu beweisen.
F\"ur die Regel \eqref{e:produkt} schreiben wir
\[
f(x+h)g(x+h) = (f(x+h)-f(x))g(x+h) + f(x) (g(x+h)-g(x))
\]
und wir nutzen das gleiche Argument f\"ur den Fall einer reellen Variabel.

Wir beweisen nun \eqref{e:bruch}.  Da $f$ stetig in $x$ ist, $f(x+h)\neq 0$ falls $\Norm{h}$ klein genug ist.
Deswegen ist $1/f$ wohldefiniert in einer Umgebung von $x$.
Das Ziel ist eine lineare Abbildung $L$ zu finden so dass
  \[\Limo{h}\frac{\frac{1}{f(x+h)}-\frac{1}{f(x)}-L(h)}{\Norm{h}}\]
wobei
\[L=-\frac{1}{f(x)^2}\md f|_x\, .\]
  Wir schreiben
\[\Limo{h}\frac{\overbrace{\frac{1}{f(x+h)}-\frac{1}{f(x)}-\frac{1}{f(x)^2}(h)\md f|_x(h)}^A}{\Norm{h}}\]
und rechnen
  \[\frac{1}{f(x+h)}-\frac{1}{f(x)}=\frac{f(x)-f(x+h)}{f(x)f(x+h)}\, .\]
Also,
 \[A=\underbrace{\frac{-(-f(x)+f(x+h)) - \md f|_x(h)}{f(x)f(x+h)}}_C
+\underbrace{\frac{-\md f|_x(h)}{f(x)f(x+h)} + \frac{\md f|_x(h)}{f(x)^2}}_B\]
Aber
\[\frac{B}{\Norm{h}}=-\underbrace{\frac{1}{f(x)f(x+h)}}_{\to f(x)^2\neq 0}
\underbrace{\frac{f(x+h)-f(x)-\md f|_x(h)}{\Norm{h}}}_{\to 0 \text{ weil $f$ diff. in $x$}}\]
und deswegen
\[\Limo{h}\frac{B}{\Norm{h}}=0\]
Ausserdem
  \[\frac{C}{\Norm{h}}=\underbrace{\frac{\md f|_x(h)}{\Norm{h}}}_{D}\frac{1}{f(x)}\underbrace{\left( \frac{1}{f(x)}-\frac{1}{f(x+h)} \right)}_{\to 0}\]
  Sei $L=\md f|_x$ und $\Norm{L}_O$ ihre Operatornorm
  \[\Abs{\md f|_x(h)}=\Abs{L(h)}\leq\Norm{L}_O\Norm{h}\]
  \[\implies D =  \frac{\Abs{\md f|_x(h)}}{\Norm{h}}\leq\Norm{L}\, .\]
Deswegen ist $D$ beschr\"ankt und 
\[
\lim_{h\to 0} \frac{C}{\Norm{h}} = 0\, .
\]
\end{Bew}
